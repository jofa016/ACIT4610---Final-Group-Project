{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "955a1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ec266",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52a72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message  \\\n",
       "0      0  Go until jurong point, crazy.. Available only ...   \n",
       "1      0                      Ok lar... Joking wif u oni...   \n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      0  U dun say so early hor... U c already then say...   \n",
       "4      0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i dont think he goes to usf he lives aroun...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "#df.head()\n",
    "\n",
    "# data cleaning function\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    #remove html tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    #remove urls\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    #remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    #remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    #remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['cleaned_message'] = df['message'].apply(clean_text)\n",
    "#df.head()\n",
    "\n",
    "# encode labels\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8956b",
   "metadata": {},
   "source": [
    "## Vectorize\n",
    "Vectorizing using TF-IDF(Term Frequency - Inverse Document Frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa8e7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix shape: (5572, 9211)\n",
      "Vocabulary size: 9211\n",
      "['jurong', 'point', 'crazy', 'available', 'bugis', 'great', 'world', 'la', 'buffet', 'cine', 'got', 'amore', 'wat', 'ok', 'lar', 'joking', 'wif', 'oni', 'free', 'entry']\n",
      "Non-zero features in first email: 13\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english'\n",
    "    #ngram_range=(1, 2),\n",
    "    #max_df=0.9,\n",
    "    #min_df=2\n",
    ")\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(df['cleaned_message'])\n",
    "y = df['label'].values\n",
    "\n",
    "print(f\"matrix shape: {X.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "# Save the vectorizer\n",
    "with open('../models/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# see a few top words\n",
    "print(list(vectorizer.vocabulary_.keys())[:20])\n",
    "\n",
    "# example vector for one email\n",
    "sample_vec = X[0]\n",
    "print(\"Non-zero features in first email:\", sample_vec.nnz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201be7a8",
   "metadata": {},
   "source": [
    "## Train NSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed5adf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating detectors...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'A'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# train NSA detector\u001b[39;00m\n\u001b[0;32m     54\u001b[0m nsa_detector \u001b[38;5;241m=\u001b[39m VDetectorNSA()\n\u001b[1;32m---> 55\u001b[0m \u001b[43mnsa_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_self_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 43\u001b[0m, in \u001b[0;36mVDetectorNSA.fit\u001b[1;34m(self, training_data)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#check similarity to self samples\u001b[39;00m\n\u001b[0;32m     42\u001b[0m sims \u001b[38;5;241m=\u001b[39m training_data \u001b[38;5;241m@\u001b[39m candidate\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(\u001b[43msims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# candidate too similar to self\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Accept the candidate\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'A'"
     ]
    }
   ],
   "source": [
    "X_self = X[y == 0]  # ham messages\n",
    "#print(f\"Self samples:{X_self.shape[0]}\") # 4825 samples\n",
    "X_nonself = X[y == 1]  # spam messages\n",
    "#print(f\"Non-self samples:{X_nonself.shape[0]}\") # 747 samples\n",
    "\n",
    "X_self_train, X_self_test = train_test_split(X_self, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define NSA detector class\n",
    "@dataclass\n",
    "class Detector:\n",
    "    vector: np.ndarray\n",
    "    radius: float\n",
    "\n",
    "class VDetectorNSA:\n",
    "    def __init__(self, minthreshold: float = 0.3, maxthreshold: float = 0.7, max_detectors: int = 300, max_trials: int = 10000):\n",
    "        self.minthreshold = minthreshold\n",
    "        self.maxthreshold = maxthreshold\n",
    "        self.max_detectors = max_detectors\n",
    "        self.max_trials = max_trials\n",
    "        self.detectors = []\n",
    "    \n",
    "    def random_vector(self, dim: int, active_k: int=10) -> np.ndarray:\n",
    "        '''Create a random sparse-like vector with `active_k` entries'''\n",
    "        idx = np.random.choice(dim, size=min(active_k, dim), replace=False)\n",
    "        vals = np.random.rand(len(idx)) + 0.1  # avoid zeros\n",
    "        v = np.zeros(dim, dtype=np.float32)\n",
    "        v[idx] = vals\n",
    "        v = v / np.linalg.norm(v)  # normalize\n",
    "        return v\n",
    "    \n",
    "    def fit(self, training_data: np.ndarray):\n",
    "        data, dim = training_data.shape\n",
    "        trials, accepted = 0, 0\n",
    "\n",
    "        print(\"Generating detectors...\")\n",
    "        while accepted < self.max_detectors and trials < self.max_trials:\n",
    "            trials += 1\n",
    "            threshold = np.random.uniform(self.minthreshold, self.maxthreshold)\n",
    "            candidate = self.random_vector(dim)\n",
    "\n",
    "            #check similarity to self samples\n",
    "            sims = training_data @ candidate\n",
    "            if np.any(sims.A.ravel() >= threshold):\n",
    "                continue  # candidate too similar to self\n",
    "\n",
    "            # Accept the candidate\n",
    "            self.detectors.append(Detector(vector=candidate, radius=threshold))\n",
    "            accepted += 1\n",
    "        \n",
    "        print(f\"Generated {accepted} detectors in {trials} trials.\")\n",
    "        return self\n",
    "\n",
    "# train NSA detector\n",
    "nsa_detector = VDetectorNSA()\n",
    "nsa_detector.fit(X_self_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c20ce8",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6ea6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(VDetectorNSA, X: np.ndarray) -> np.ndarray:\n",
    "    if not VDetectorNSA.detectors:\n",
    "        raise ValueError(\"Detector has not been fitted with training data.\")\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    preds = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    #Stack all detector vectors for efficient computation\n",
    "    D = np.vstack([d.vector for d in VDetectorNSA.detectors])\n",
    "    radii = np.array([d.radius for d in VDetectorNSA.detectors])[None, :]\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    batch_size = 256 # process in batches to save memory\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        X_batch = X[start:end]\n",
    "\n",
    "        sims = X_batch @ D.T  # shape (batch_size, n_detectors)\n",
    "        sims = sims.A  if hasattr(sims, 'A') else sims  # convert to dense if sparse\n",
    "\n",
    "        # if similarity exceeds any detector's radius, mark as non-self (spam)\n",
    "        hits = (sims >= radii)\n",
    "        preds[start:end] = np.any(hits, axis=1).astype(int)\n",
    "    return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07525ffd",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = vstack([X_self_test, X_nonself])\n",
    "y_eval = np.hstack([\n",
    "    np.zeros(X_self_test.shape[0], dtype=int),\n",
    "    np.ones(X_nonself.shape[0], dtype=int)\n",
    "]\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
