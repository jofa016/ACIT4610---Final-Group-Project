{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "955a1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data manipulation\n",
    "import numpy as np  # for numeric operations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # for text to vector conversion\n",
    "from sklearn.model_selection import train_test_split # for splitting data\n",
    "import re                    # for regex operations\n",
    "import scipy.sparse as sp # for sparse matrix operations\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.sparse import vstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ec266",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cb52a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "#df.head()\n",
    "\n",
    "# data cleaning function\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    #remove html tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    #remove urls\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    #remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    #remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    #remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['cleaned_message'] = df['message'].apply(clean_text)\n",
    "#df.head()\n",
    "\n",
    "# encode labels\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8956b",
   "metadata": {},
   "source": [
    "## Vectorize\n",
    "Vectorizing using TF-IDF(Term Frequency - Inverse Document Frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fa8e7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (5572, 14738)\n",
      "Binary density: 0.001315\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the messages using TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    #stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.9,\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df['cleaned_message'])\n",
    "y = df['label'].values\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
    "\n",
    "# Convert to binary inorder to use with NSA\n",
    "tau = 0.05\n",
    "X_binary = (X_tfidf >= tau).astype(np.uint8)\n",
    "\n",
    "#basic diagnostics\n",
    "density = X_binary.nnz / (X_binary.shape[0] * X_binary.shape[1])\n",
    "print(f\"Binary density: {density:6f}\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbff67",
   "metadata": {},
   "source": [
    "- as expected we got total of 5572 messages\n",
    "- we have a feature space of 14738 with 0.0013 density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201be7a8",
   "metadata": {},
   "source": [
    "## Train and predict NSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "240e5e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham matrix shape: (4825, 14738)\n",
      "Spam matrix shape: (747, 14738)\n"
     ]
    }
   ],
   "source": [
    "# seperate self(ham) and non-self (spam) \n",
    "X_ham = X_binary[y == 0]\n",
    "X_spam = X_binary[y == 1]\n",
    "\n",
    "print(f\"Ham matrix shape: {X_ham.shape}\")\n",
    "print(f\"Spam matrix shape: {X_spam.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1489b4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham train shape: (3860, 14738)\n",
      "Ham test shape: (965, 14738)\n"
     ]
    }
   ],
   "source": [
    "# split self data into train and test sets\n",
    "X_ham_train, X_ham_test = train_test_split(X_ham, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Ham train shape: {X_ham_train.shape}\")\n",
    "print(f\"Ham test shape: {X_ham_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "29c2b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "# we decide to implement NSA using r-overlap matching rule between message and detector \n",
    "\n",
    "class Detector:\n",
    "    idx: np.ndarray #active bits indices\n",
    "    radius: int   #required overlap \n",
    "\n",
    "class VDetectorNSA_Binary:\n",
    "\n",
    "    def __init__(self,\n",
    "                 k: int,  #number of detectors\n",
    "                 r_min: int, #min radius\n",
    "                 r_max: int, #max radius\n",
    "                 max_detectors: int, #maximum number of detectors to generate\n",
    "                 max_tries: int, #maximum number of tries to generate detectors\n",
    "                 batch_size: int, #batch size for detector generation\n",
    "                 sampling: str = 'antiprofile', #sampling method\n",
    "                 random_state: int = 42 #random seed\n",
    "                 ):\n",
    "        self.k = k\n",
    "        self.r_min = r_min\n",
    "        self.r_max = r_max\n",
    "        self.max_detectors = max_detectors\n",
    "        self.max_tries = max_tries\n",
    "        self.batch_size = batch_size \n",
    "        self.sampling = sampling\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.dim = None\n",
    "        self.detectors : list[Detector] = []\n",
    "        self.p_detect = None\n",
    "\n",
    "    def _build_antiprofile_probs(self, X_ham_train: sp.csr_matrix) -> np.ndarray:\n",
    "        \"\"\"Build antiprofile sampling probabilities from self data.\"\"\"\n",
    "        #calculate feature frequencies\n",
    "        assert sp.issparse(X_ham_train), \"Input data must be a sparse matrix.\"\n",
    "        p_ham = (X_ham_train.sum(axis=0) / X_ham_train.shape[0]).A1\n",
    "        p = np.clip(1.0 - p_ham, 1e-8, 1.0)\n",
    "        self.p_detect = p / p.sum()  # avoid zero probabilities\n",
    "    \n",
    "    def _sample_indices(self):\n",
    "        if self.sampling == 'antiprofile':\n",
    "            return np.sort(np.random.choice(\n",
    "                self.dim, size=min(self.k, self.dim), replace=False, p=self.p_detect))\n",
    "        else:  \n",
    "            return np.sort(np.random.choice(\n",
    "                self.dim, size=min(self.k, self.dim), replace=False))\n",
    "        \n",
    "    @staticmethod\n",
    "    def _vec_from_idx(idx, dim: int) -> sp.csr_matrix:\n",
    "        data = np.ones(len(idx), dtype=np.uint8)\n",
    "        rows = np.zeros(len(idx), dtype=np.int32)\n",
    "        return sp.csr_matrix((data, (rows, idx)), shape=(1, dim))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _max_overlap(X_binary: sp.csr_matrix, detector_vec: sp.csr_matrix, batch_size: int) -> np.ndarray:\n",
    "        \"\"\"Compute maximum overlap between each row in X_binary and the detector vector.\"\"\"\n",
    "        n = X_binary.shape[0]\n",
    "        best = 0\n",
    "        for s in range(0, n, batch_size):\n",
    "            e = min(s + batch_size, n)\n",
    "            overlap = (X_binary[s:e] @ detector_vec.T).A.ravel()\n",
    "            if overlap.size:\n",
    "                m = int(overlap.max())\n",
    "                if m > best:\n",
    "                    best = m\n",
    "\n",
    "        return best\n",
    "    \n",
    "    def fit(self, X_ham_train: sp.csr_matrix):\n",
    "        assert sp.issparse(X_ham_train), \"Input data must be a sparse matrix.\"\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        self.dim = X_ham_train.shape[1]\n",
    "        if self.sampling == 'antiprofile':\n",
    "            self._build_antiprofile_probs(X_ham_train)\n",
    "        \n",
    "        accepted, tries = 0, 0\n",
    "        while accepted < self.max_detectors and tries < self.max_tries:\n",
    "            tries += 1\n",
    "            idx = self._sample_indices()\n",
    "            det_vec = self._vec_from_idx(idx, self.dim)\n",
    "\n",
    "            m_o = self._max_overlap(X_ham_train, det_vec, self.batch_size)\n",
    "            r = min(self.r_max, max(self.r_min, m_o -1))\n",
    "\n",
    "            if r < self.r_min:\n",
    "                continue # too close to self data\n",
    "\n",
    "            # ensure no ham at or above r\n",
    "            if m_o >= r:\n",
    "                pass\n",
    "\n",
    "            self.detectors.append(Detector(idx=idx, radius=r))\n",
    "            accepted += 1\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_bin: sp, k_hits: int = 1) -> np.ndarray:\n",
    "        \"\"\"Predict if samples in X_bin are non-self (spam) or self (ham).\"\"\"\n",
    "        assert self.detectors, \"The model has not been fitted yet.\"\n",
    "        n = X_bin.shape[0]\n",
    "        predictions = np.zeros(n, dtype=np.uint8)  # default to self (ham)\n",
    "\n",
    "        # build detector matrix once for \n",
    "        rows, cols, data, r_list = [], [], [], []\n",
    "        for i, det in enumerate(self.detectors):\n",
    "            cols.extend(det.idx.tolist())\n",
    "            rows.extend([i] * len(det.idx))\n",
    "            data.extend([1] * len(det.idx))\n",
    "            r_list.append(det.radius)\n",
    "        det_matrix = sp.csr_matrix((data, (rows, cols)), shape=(len(self.detectors), self.dim))\n",
    "        r_array = np.array(r_list)\n",
    "\n",
    "        for start in range(0, n, self.batch_size):\n",
    "            end = min(start + self.batch_size, n)\n",
    "            overlaps = (X_bin[start:end] @ det_matrix.T).A\n",
    "            hits = (overlaps >= r_array)\n",
    "            if k_hits == 1:\n",
    "                predictions[start:end] = hits.any(axis=1).astype(np.uint8)\n",
    "            else:\n",
    "                predictions[start:end] = (hits.sum(axis=1) >= k_hits).astype(np.uint8)\n",
    "        return predictions\n",
    "                                       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07525ffd",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6c5d0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsa = VDetectorNSA_Binary(\n",
    "    k=60,\n",
    "    r_min=2, r_max=8,\n",
    "    max_detectors=2000,\n",
    "    max_tries=50000,\n",
    "    sampling=\"antiprofile\",\n",
    "    random_state=42,\n",
    "    batch_size=1000\n",
    ").fit(X_ham_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "889cab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build eval set: ham_test + all spam\n",
    "X_eval = vstack([X_ham_test, X_spam])\n",
    "y_eval = np.hstack([\n",
    "    np.zeros(X_ham_test.shape[0], dtype=np.uint8),\n",
    "    np.ones(X_spam.shape[0], dtype=np.uint8)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a194e25",
   "metadata": {},
   "source": [
    "### Visualization of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ec8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cd181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
