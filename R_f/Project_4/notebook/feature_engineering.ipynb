{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40612c66",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Toc\"><a>\n",
    "# Table of Content\n",
    "1. [Install libraries](#install_libraries)\n",
    "1. [Import libraries](#import_libraries)\n",
    "1. [Load and clean dataset](#load_and_clean_dataset)\n",
    "1. [Split data, Vectorize(TF-IDF) and Binarize](#split_vectorize_&_binarize)\n",
    "1. [Implement NSA](#evaluation)\n",
    "1. [Validation](#validation)\n",
    "1. [Evaluate](#optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184de442",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"install_libraries\"></a>\n",
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fe6a822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.1 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.10 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: matplotlib>=3.8 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 5)) (3.9.2)\n",
      "Requirement already satisfied: seaborn>=0.13 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from pandas>=2.1->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from pandas>=2.1->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from pandas>=2.1->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from scikit-learn>=1.3->-r ../requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from scikit-learn>=1.3->-r ../requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (3.1.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.1->-r ../requirements.txt (line 1)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install requirements from requirements.txt\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773815f2",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"import_libraries\"></a>\n",
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "955a1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Machine learning & text processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_curve, f1_score, auc\n",
    ")\n",
    "\n",
    "# Sparse matrix operations\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utility\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ec266",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"load_and_clean_dataset\"></a>\n",
    "# Load and clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb52a72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                            message  \\\n",
      "12       1  URGENT! You have won a 1 week FREE membership ...   \n",
      "15       1  XXXMobileMovieClub: To use your credit, click ...   \n",
      "135      1  Want 2 get laid tonight? Want real Dogging loc...   \n",
      "136      0             I only haf msn. It's yijue@hotmail.com   \n",
      "164      1  -PLS STOP bootydelious (32/F) is inviting you ...   \n",
      "\n",
      "                                       cleaned_message  \\\n",
      "12   urgent you have won a 1 week free membership i...   \n",
      "15   xxxmobilemovieclub to use your credit click th...   \n",
      "135  want 2 get laid tonight want real dogging loca...   \n",
      "136                                i only haf msn it s   \n",
      "164  pls stop bootydelious 32 f is inviting you to ...   \n",
      "\n",
      "                        cleaned_message_with_url_email  \n",
      "12   urgent you have won a 1 week free membership i...  \n",
      "15   xxxmobilemovieclub to use your credit click th...  \n",
      "135  want 2 get laid tonight want real dogging loca...  \n",
      "136              i only haf msn it s yijue hotmail com  \n",
      "164  pls stop bootydelious 32 f is inviting you to ...  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "#df.head()\n",
    "\n",
    "# data cleaning function\n",
    "'''we will remove html tags, urls, punctuation, numbers, emails, and extra whitespace.\n",
    "but we will keep the option to not remove urls and emails if needed in future because they might be relevant for spam detection.'''\n",
    "\n",
    "def clean_text(text: str, replace_url=True, replace_email=True):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # replace URLs\n",
    "    if replace_url:\n",
    "        text = re.sub(r'http\\S+|www\\S+', ' <URL> ', text)\n",
    "\n",
    "    # replace emails\n",
    "    if replace_email:\n",
    "        text = re.sub(r'\\S+@\\S+', ' <EMAIL> ', text)\n",
    "\n",
    "    # remove html tags (very rare in SMS)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "    # keep numbers, letters, placeholders\n",
    "    text = re.sub(r'[^a-z0-9<> ]+', ' ', text)\n",
    "\n",
    "    # collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# apply cleaning\n",
    "\n",
    "df['cleaned_message'] = df['message'].apply(clean_text)\n",
    "\n",
    "df['cleaned_message_with_url_email'] = df['message'].apply(lambda x: clean_text(x, replace_url=False, replace_email=False))\n",
    "# encode labels\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# see the cleaned data and filter rows where cleaning made a difference to check the effectiveness of cleaning\n",
    "\n",
    "mask = (\n",
    "    (df['message'] != df['cleaned_message']) &\n",
    "    (df['message'] != df['cleaned_message_with_url_email']) &\n",
    "    (df['cleaned_message'] != df['cleaned_message_with_url_email'])\n",
    ")\n",
    "\n",
    "df_filtered = df[mask]\n",
    "print(df_filtered.head(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9a8d6",
   "metadata": {},
   "source": [
    "We checked if the cleaning mechanism is working and it is working as we want in message 136 the email is removed in cleaned_messages while in cleaned_message_with_url it is preserved and sanitized and that is exactly how we want this column to behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8956b",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"split_vectorize_&_binarize\"></a>\n",
    "# Split data, Vectorize (TF-IDF) and Binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bfdec5",
   "metadata": {},
   "source": [
    "We want to split the data into training(60% of ham) and the rest of ham and spam into half for validation and half for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "083f627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ham messages: 4825\n",
      "Total spam messages: 747\n",
      "Ham train: 2895, Ham val: 965, Ham test: 965\n",
      "Spam val: 373, Spam test: 374\n"
     ]
    }
   ],
   "source": [
    "# Separete ham and spam\n",
    "ham = df[df['label'] == 0]['cleaned_message']\n",
    "spam = df[df['label'] == 1]['cleaned_message']\n",
    "# those with url and email preserved\n",
    "ham_with_url_email = df[df['label'] == 0]['cleaned_message_with_url_email']\n",
    "spam_with_url_email = df[df['label'] == 1]['cleaned_message_with_url_email']\n",
    "\n",
    "print(f\"Total ham messages: {len(ham)}\")\n",
    "print(f\"Total spam messages: {len(spam)}\")\n",
    "\n",
    "# split ham\n",
    "def split_ham(ham: pd.Series, train_size=0.6, test_size= 0.5):\n",
    "    ham_train, ham_temp = train_test_split(ham, train_size=train_size, random_state=42)\n",
    "    ham_val, ham_test = train_test_split(ham_temp, test_size=test_size, random_state=42)\n",
    "    return ham_train, ham_val, ham_test\n",
    "\n",
    "ham_train, ham_val, ham_test = split_ham(ham)\n",
    "ham_train_with_url_email, ham_val_with_url_email, ham_test_with_url_email = split_ham(ham_with_url_email)\n",
    "\n",
    "# split spam\n",
    "def split_spam(spam: pd.Series, test_size=0.5):\n",
    "    spam_val, spam_test = train_test_split(spam, test_size=test_size, random_state=42)\n",
    "    return spam_val, spam_test\n",
    "spam_val, spam_test = split_spam(spam)\n",
    "spam_val_with_url_email, spam_test_with_url_email = split_spam(spam_with_url_email)\n",
    "\n",
    "print(f\"Ham train: {len(ham_train)}, Ham val: {len(ham_val)}, Ham test: {len(ham_test)}\")\n",
    "print(f\"Spam val: {len(spam_val)}, Spam test: {len(spam_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048dd420",
   "metadata": {},
   "source": [
    "To vectorize the message we chose to use TfidfVectorizer which is a scikit-learn tool that turns a pile of text into numbers our model will be able to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fa8e7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (plain): 7012\n",
      "Train matrix shape (plain): (2895, 7012)\n",
      "Vocabulary (url_email): 7012\n",
      "Train matrix shape (url_email): (2895, 7012)\n"
     ]
    }
   ],
   "source": [
    "# fit vectorizer only on ham_train using the TF-IDF vectorizer \n",
    "'''we chose decide to change som of the parameters of the TfidfVectorizer to better suit our SMS spam detection task:\n",
    "- ngram_range=(1, 2): This allows the model to capture both unigrams and bigrams, which can be useful for understanding context in short messages.\n",
    "- max_df=0.95: This setting ignores terms that appear in more than 95% of the documents, helping to eliminate very common words that may not be informative.\n",
    "- min_df=2: This setting ignores terms that appear in less than 2 documents, which helps to reduce noise from very rare words that may not contribute significantly to the model's performance.\n",
    "- sublinear_tf=True: This applies sublinear term frequency scaling, which can help to moderate the impact of very frequent terms.\n",
    "'''\n",
    "# Vectorizer for normal cleaned messages\n",
    "vectorizer_plain = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "Xh_train_plain = vectorizer_plain.fit_transform(ham_train)\n",
    "Xh_val_plain   = vectorizer_plain.transform(ham_val)\n",
    "Xh_test_plain  = vectorizer_plain.transform(ham_test)\n",
    "Xs_val_plain   = vectorizer_plain.transform(spam_val)\n",
    "Xs_test_plain  = vectorizer_plain.transform(spam_test)\n",
    "\n",
    "print(\"Vocabulary (plain):\", len(vectorizer_plain.vocabulary_))\n",
    "print(\"Train matrix shape (plain):\", Xh_train_plain.shape)\n",
    "\n",
    "# Vectorizer for messages with URL/email preserved\n",
    "vectorizer_url = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "Xh_train_url = vectorizer_url.fit_transform(ham_train_with_url_email)\n",
    "Xh_val_url   = vectorizer_url.transform(ham_val_with_url_email)\n",
    "Xh_test_url  = vectorizer_url.transform(ham_test_with_url_email)\n",
    "Xs_val_url   = vectorizer_url.transform(spam_val_with_url_email)\n",
    "Xs_test_url  = vectorizer_url.transform(spam_test_with_url_email)\n",
    "\n",
    "print(\"Vocabulary (url_email):\", len(vectorizer_url.vocabulary_))\n",
    "print(\"Train matrix shape (url_email):\", Xh_train_url.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa116bd",
   "metadata": {},
   "source": [
    "The vocabulary is identical so we check if there is enough difference between the one will url and they one that dont. Other than that, 7012 feature spaces is quit good . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32e6be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 messages with URLs\n",
      "36 messages with emails\n",
      "126 messages differ\n"
     ]
    }
   ],
   "source": [
    "df['has_url'] = df['message'].str.contains(r'http|www', case=False, na=False)\n",
    "df['has_email'] = df['message'].str.contains(r'@', na=False)\n",
    "\n",
    "print(df['has_url'].sum(), \"messages with URLs\")\n",
    "print(df['has_email'].sum(), \"messages with emails\")\n",
    "\n",
    "diff = df[df['cleaned_message'] != df['cleaned_message_with_url_email']]\n",
    "print(len(diff), \"messages differ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcdcd3f",
   "metadata": {},
   "source": [
    "it makes sense now since there is difference only in 126 messages only the is no big difference but we are going with the one with url just to make sure we take advantage of the presence of emails and urls.\n",
    "Now we will binarize these because detectors operate on binary patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2a2c66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarized train matrix shape (url_email): (2895, 7012)\n"
     ]
    }
   ],
   "source": [
    "# binarization function\n",
    "def binarize_matrix(X: sp.csr_matrix, threshold=0.2) -> sp.csr_matrix:\n",
    "    '''convert sparse matrix to binary based on threshold\n",
    "    we chose 0.2 but we will experiment with different thresholds later '''\n",
    "    X_bin = X.copy()\n",
    "    X_bin.data = np.where(X_bin.data > threshold, 1, 0)\n",
    "    X_bin.eliminate_zeros()\n",
    "    return X_bin\n",
    "\n",
    "# apply binarization\n",
    "Xh_train_url_bin = binarize_matrix(Xh_train_url)\n",
    "Xh_val_url_bin   = binarize_matrix(Xh_val_url)\n",
    "Xh_test_url_bin  = binarize_matrix(Xh_test_url)\n",
    "Xs_val_url_bin   = binarize_matrix(Xs_val_url)\n",
    "Xs_test_url_bin  = binarize_matrix(Xs_test_url)\n",
    "\n",
    "# sanity check\n",
    "print(\"Binarized train matrix shape (url_email):\", Xh_train_url_bin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51caf2d8",
   "metadata": {},
   "source": [
    "the shapes match our TF-IDF shapes so that is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201be7a8",
   "metadata": {},
   "source": [
    "## Train and predict NSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29c2b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Detector:\n",
    "    idx: np.ndarray   # active bits indices\n",
    "    radius: int       # required overlap threshold\n",
    "    \n",
    "class VDetectorNSA_Binary:\n",
    "    \"\"\"\n",
    "    Binary Negative Selection Algorithm using r-overlap matching rule.\n",
    "    Detectors are binary sparse vectors that should NOT match any self sample\n",
    "    with overlap >= radius.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 k: int,                 # active bits per detector\n",
    "                 r_min: int, r_max: int, # min/max overlap radius\n",
    "                 max_detectors: int,     # how many detectors to accept\n",
    "                 max_tries: int,         # max attempts to generate\n",
    "                 batch_size: int,        # batch size for overlap calc\n",
    "                 sampling: str = \"antiprofile\",\n",
    "                 random_state: int = 42):\n",
    "        self.k = k\n",
    "        self.r_min = r_min\n",
    "        self.r_max = r_max\n",
    "        self.max_detectors = max_detectors\n",
    "        self.max_tries = max_tries\n",
    "        self.batch_size = batch_size\n",
    "        self.sampling = sampling\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.dim = None\n",
    "        self.detectors: list[Detector] = []\n",
    "        self.p_detect = None\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # internal helpers\n",
    "    # --------------------------------------------------------------\n",
    "    def _build_antiprofile_probs(self, X_ham_train: sp.csr_matrix) -> None:\n",
    "        \"\"\"Build antiprofile sampling probabilities from self data.\"\"\"\n",
    "        assert sp.issparse(X_ham_train)\n",
    "        p_ham = (X_ham_train.sum(axis=0) / X_ham_train.shape[0]).A1\n",
    "        p = np.clip(1.0 - p_ham, 1e-8, 1.0)\n",
    "        self.p_detect = p / p.sum()\n",
    "\n",
    "    def _sample_indices(self) -> np.ndarray:\n",
    "        if self.sampling == \"antiprofile\" and self.p_detect is not None:\n",
    "            return np.sort(np.random.choice(self.dim, size=min(self.k, self.dim),\n",
    "                                            replace=False, p=self.p_detect))\n",
    "        return np.sort(np.random.choice(self.dim, size=min(self.k, self.dim),\n",
    "                                        replace=False))\n",
    "\n",
    "    @staticmethod\n",
    "    def _vec_from_idx(idx: np.ndarray, dim: int) -> sp.csr_matrix:\n",
    "        \"\"\"Construct a 1×dim sparse row vector with 1s at idx.\"\"\"\n",
    "        data = np.ones(len(idx), dtype=np.uint8)\n",
    "        rows = np.zeros(len(idx), dtype=np.int32)\n",
    "        return sp.csr_matrix((data, (rows, idx)), shape=(1, dim))\n",
    "\n",
    "    @staticmethod\n",
    "    def _max_overlap(X: sp.csr_matrix, detector_vec: sp.csr_matrix, batch_size: int) -> int:\n",
    "        \"\"\"Return the maximum overlap between detector and any row in X.\"\"\"\n",
    "        best = 0\n",
    "        n = X.shape[0]\n",
    "        for s in range(0, n, batch_size):\n",
    "            e = min(s + batch_size, n)\n",
    "            overlap = (X[s:e] @ detector_vec.T).A.ravel()\n",
    "            if overlap.size:\n",
    "                best = max(best, int(overlap.max()))\n",
    "        return best\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # main API\n",
    "    # --------------------------------------------------------------\n",
    "    def fit(self, X_ham_train: sp.csr_matrix):\n",
    "        assert sp.issparse(X_ham_train)\n",
    "        self.dim = X_ham_train.shape[1]\n",
    "        if self.sampling == \"antiprofile\":\n",
    "            self._build_antiprofile_probs(X_ham_train)\n",
    "\n",
    "        accepted, tries = 0, 0\n",
    "        while accepted < self.max_detectors and tries < self.max_tries:\n",
    "            tries += 1\n",
    "            idx = self._sample_indices()\n",
    "            det_vec = self._vec_from_idx(idx, self.dim)\n",
    "\n",
    "            # choose a random radius within the allowed range\n",
    "            r = np.random.randint(self.r_min, self.r_max + 1)\n",
    "\n",
    "            # reject if detector matches any ham within radius\n",
    "            m_o = self._max_overlap(X_ham_train, det_vec, self.batch_size)\n",
    "            if m_o >= r:      # too similar to self, reject\n",
    "                continue\n",
    "\n",
    "            # accept detector\n",
    "            self.detectors.append(Detector(idx=idx, radius=r))\n",
    "            accepted += 1\n",
    "\n",
    "        print(f\"Generated {accepted} detectors after {tries} tries.\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_bin: sp.csr_matrix, k_hits: int = 1, return_score: bool = False):\n",
    "        \"\"\"Predict non-self (1=spam) vs self (0=ham).\"\"\"\n",
    "        assert self.detectors, \"Model not fitted.\"\n",
    "        n = X_bin.shape[0]\n",
    "        predictions = np.zeros(n, dtype=np.uint8)\n",
    "\n",
    "        # build detector matrix\n",
    "        rows, cols, data, r_list = [], [], [], []\n",
    "        for i, det in enumerate(self.detectors):\n",
    "            cols.extend(det.idx.tolist())\n",
    "            rows.extend([i] * len(det.idx))\n",
    "            data.extend([1] * len(det.idx))\n",
    "            r_list.append(det.radius)\n",
    "        det_matrix = sp.csr_matrix((data, (rows, cols)), shape=(len(self.detectors), self.dim))\n",
    "        r_array = np.array(r_list)\n",
    "\n",
    "        scores = np.zeros(n, dtype=np.int32) if return_score else None\n",
    "        for s in range(0, n, self.batch_size):\n",
    "            e = min(s + self.batch_size, n)\n",
    "            overlaps = (X_bin[s:e] @ det_matrix.T).A\n",
    "            hits = (overlaps >= r_array)\n",
    "            if return_score:\n",
    "                scores[s:e] = hits.sum(axis=1)\n",
    "            if k_hits == 1:\n",
    "                predictions[s:e] = hits.any(axis=1).astype(np.uint8)\n",
    "            else:\n",
    "                predictions[s:e] = (hits.sum(axis=1) >= k_hits).astype(np.uint8)\n",
    "        return predictions if not return_score else (predictions, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07525ffd",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a86f4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to binarize sparse matrix\n",
    "def binarize_sparse_matrix(X: sp.csr_matrix, tau: float= 0.05) -> sp.csr_matrix:\n",
    "    '''\n",
    "    Binarize sparse matrix X using threshold tau.\n",
    "    Values greater than tau are set to 1, others to 0.\n",
    "    '''\n",
    "    assert sp.issparse(X), \"Input matrix must be a sparse matrix.\"\n",
    "    X_bin = X.copy()\n",
    "    X_bin.data = np.where(X_bin.data > tau, 1, 0)\n",
    "    return X_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7899c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xh_train_bin = binarize_sparse_matrix(Xh_train_tfidf, tau=0.02)\n",
    "Xh_val_bin = binarize_sparse_matrix(Xh_val_tfidf, tau=0.02)\n",
    "Xh_test_bin = binarize_sparse_matrix(Xh_test_tfidf, tau=0.02)\n",
    "Xs_val_bin = binarize_sparse_matrix(Xs_val_tfidf, tau=0.02)\n",
    "Xs_test_bin = binarize_sparse_matrix(Xs_test_tfidf, tau=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c5d0b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2000 detectors after 5783 tries.\n"
     ]
    }
   ],
   "source": [
    "nsa = VDetectorNSA_Binary(\n",
    "    k=20,\n",
    "    r_min=1, r_max=3,\n",
    "    max_detectors=2000,\n",
    "    max_tries=50000,\n",
    "    sampling=\"antiprofile\",\n",
    "    random_state=42,\n",
    "    batch_size=1000\n",
    ").fit(Xh_train_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95b1bc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#detectors: 2000  | dim: 7012\n",
      "Mean ham-hit rate per detector: 0.000021\n",
      "Mean spam-hit rate per detector: 0.000052\n",
      "HAM coverage(any hit):  0.0394\n",
      "SPAM coverage(any hit): 0.0965\n"
     ]
    }
   ],
   "source": [
    "def nsa_diagnostics(nsa, Xh_val_bin, Xs_val_bin):\n",
    "    print(f\"#detectors: {len(nsa.detectors)}  | dim: {nsa.dim}\")\n",
    "\n",
    "    # per-detector stats on validation\n",
    "    rows, cols, data, r_list = [], [], [], []\n",
    "    for i, det in enumerate(nsa.detectors):\n",
    "        cols.extend(det.idx.tolist())\n",
    "        rows.extend([i]*len(det.idx))\n",
    "        data.extend([1]*len(det.idx))\n",
    "        r_list.append(det.radius)\n",
    "    D = sp.csr_matrix((data, (rows, cols)), shape=(len(nsa.detectors), nsa.dim))\n",
    "    r = np.array(r_list)\n",
    "\n",
    "    # how many ham/spam each detector hits\n",
    "    ham_hits = (Xh_val_bin @ D.T).A >= r\n",
    "    spam_hits = (Xs_val_bin @ D.T).A >= r\n",
    "    print(f\"Mean ham-hit rate per detector: {ham_hits.mean():.6f}\")\n",
    "    print(f\"Mean spam-hit rate per detector: {spam_hits.mean():.6f}\")\n",
    "\n",
    "    # coverage: fraction of samples hit by at least one detector\n",
    "    ham_cov = ham_hits.any(axis=1).mean()\n",
    "    spam_cov = spam_hits.any(axis=1).mean()\n",
    "    print(f\"HAM coverage(any hit):  {ham_cov:.4f}\")\n",
    "    print(f\"SPAM coverage(any hit): {spam_cov:.4f}\")\n",
    "nsa_diagnostics(nsa, Xh_val_bin, Xs_val_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "889cab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----Validation phase----- \n",
    "we will tune 3 parameters here: k_hits, tau, k and r'''\n",
    "X_eval = sp.vstack([Xh_val_bin, Xs_val_bin])\n",
    "y_val = np.hstack([\n",
    "    np.zeros(Xh_val_bin.shape[0], dtype=np.uint8),\n",
    "    np.ones(Xs_val_bin.shape[0], dtype=np.uint8)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6e844f3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m best_f1, best_hits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hits \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m10\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnsa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_hits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     report \u001b[38;5;241m=\u001b[39m classification_report(y_val, y_pred, output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[83], line 119\u001b[0m, in \u001b[0;36mVDetectorNSA_Binary.predict\u001b[1;34m(self, X_bin, k_hits, return_score)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[0;32m    118\u001b[0m     e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(s \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, n)\n\u001b[1;32m--> 119\u001b[0m     overlaps \u001b[38;5;241m=\u001b[39m (\u001b[43mX_bin\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdet_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\u001b[38;5;241m.\u001b[39mA\n\u001b[0;32m    120\u001b[0m     hits \u001b[38;5;241m=\u001b[39m (overlaps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m r_array)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_score:\n",
      "File \u001b[1;32mc:\\Users\\welde\\miniconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:695\u001b[0m, in \u001b[0;36m_spbase.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScalar operands are not allowed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    694\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\welde\\miniconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:603\u001b[0m, in \u001b[0;36m_spbase._matmul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(other):\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 603\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension mismatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot yet multiply a 1d sparse array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "best_f1, best_hits = 0, None\n",
    "for hits in [1, 2, 3, 5, 7, 10]:\n",
    "    y_pred = nsa.predict(X_val, k_hits=hits)\n",
    "    report = classification_report(y_val, y_pred, output_dict=True)\n",
    "    f1 = report['1']['f1-score']\n",
    "    print(f\"k_hits={hits}: F1-score={f1:.4f}\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_hits = hits\n",
    "print(f\"Best k_hits: {best_hits} with F1-score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a194e25",
   "metadata": {},
   "source": [
    "### Visualization of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ec8ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Model not fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# === Predict ===\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred, scores \u001b[38;5;241m=\u001b[39m \u001b[43mnsa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_hits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_hits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# === Confusion Matrix ===\u001b[39;00m\n\u001b[0;32m      5\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_val, y_pred)\n",
      "Cell \u001b[1;32mIn[65], line 102\u001b[0m, in \u001b[0;36mVDetectorNSA_Binary.predict\u001b[1;34m(self, X_bin, k_hits, return_score)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_bin: sp\u001b[38;5;241m.\u001b[39mcsr_matrix, k_hits: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, return_score: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict non-self (1=spam) vs self (0=ham).\"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetectors, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel not fitted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m     n \u001b[38;5;241m=\u001b[39m X_bin\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    104\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Model not fitted."
     ]
    }
   ],
   "source": [
    "# === Predict ===\n",
    "y_pred, scores = nsa.predict(X_eval, k_hits=best_hits, return_score=True)\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=[\"True Ham\", \"True Spam\"],\n",
    "                     columns=[\"Pred Ham\", \"Pred Spam\"])\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (k_hits=1)\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Classification Report ===\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred, target_names=[\"Ham\", \"Spam\"]))\n",
    "\n",
    "# === Precision–Recall Curve ===\n",
    "prec, rec, _ = precision_recall_curve(y_val, scores)\n",
    "pr_auc = auc(rec, prec)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(rec, prec, color=\"darkorange\", linewidth=2,\n",
    "         label=f\"PR Curve (AUC = {pr_auc:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve (Spam as Positive)\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPrecision–Recall AUC: {pr_auc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
