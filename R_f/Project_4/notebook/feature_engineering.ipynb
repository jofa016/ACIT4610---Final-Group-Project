{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40612c66",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Toc\"><a>\n",
    "# Table of Content\n",
    "1. [Install libraries](#install_libraries)\n",
    "1. [Import libraries](#import_libraries)\n",
    "1. [Load and clean dataset](#create_and_inspect)\n",
    "1. [Vectorize(TF-IDF) and split](#q_learning_implementation)\n",
    "1. [Binarize TF-IDF](#q_learning_training)\n",
    "1. [Implement NSA](#evaluation)\n",
    "1. [Validation](#validation)\n",
    "1. [Evaluate](#optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184de442",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"install_libraries\"></a>\n",
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe6a822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.1 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 1)) (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.26 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.10 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: matplotlib>=3.8 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 5)) (3.9.2)\n",
      "Requirement already satisfied: seaborn>=0.13 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from -r ../requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from pandas>=2.1->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from pandas>=2.1->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from pandas>=2.1->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from scikit-learn>=1.3->-r ../requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from scikit-learn>=1.3->-r ../requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from matplotlib>=3.8->-r ../requirements.txt (line 5)) (3.1.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\welde\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.1->-r ../requirements.txt (line 1)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# install requirements from requirements.txt\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773815f2",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"import_libraries\"></a>\n",
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "955a1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Machine learning & text processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_curve, f1_score, auc\n",
    ")\n",
    "\n",
    "# Sparse matrix operations\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utility\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ec266",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb52a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "#df.head()\n",
    "\n",
    "# data cleaning function\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    #remove html tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    #remove urls\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    #remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    #remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    #remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['cleaned_message'] = df['message'].apply(clean_text)\n",
    "#df.head()\n",
    "\n",
    "# encode labels\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8956b",
   "metadata": {},
   "source": [
    "## Vectorize\n",
    "Vectorizing using TF-IDF(Term Frequency - Inverse Document Frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083f627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels first in order to build a proper train/val/test split\n",
    "train_idx = (df['label'] == 0)\n",
    "ham = df[df['label'] == 0]\n",
    "spam = df[df['label'] == 1]\n",
    "\n",
    "# split ham into 60% train, 20% val, 20% test\n",
    "ham_train, ham_temp = train_test_split(ham['cleaned_message'], test_size=0.4, random_state=42)\n",
    "ham_val, ham_test = train_test_split(ham_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# split spam into val and test (50% each)\n",
    "spam_val, spam_test = train_test_split(spam['cleaned_message'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa8e7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit vectorizer only on ham_train\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='char_wb',  # character n-grams with word boundaries\n",
    "    ngram_range=(3, 5),  # trigrams to 5-grams\n",
    "    max_df=0.95,         # ignore terms that appear in more than 95% of documents\n",
    "    min_df=2             # ignore terms that appear in less than 2 documents\n",
    ")\n",
    "Xh_train_tfidf = vectorizer.fit_transform(ham_train)\n",
    "Xh_val_tfidf = vectorizer.transform(ham_val)\n",
    "Xh_test_tfidf = vectorizer.transform(ham_test)\n",
    "Xs_val_tfidf = vectorizer.transform(spam_val)\n",
    "Xs_test_tfidf = vectorizer.transform(spam_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbff67",
   "metadata": {},
   "source": [
    "- as expected we got total of 5572 messages\n",
    "- we have a feature space of 14738 with 0.0013 density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201be7a8",
   "metadata": {},
   "source": [
    "## Train and predict NSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29c2b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Detector:\n",
    "    idx: np.ndarray   # active bits indices\n",
    "    radius: int       # required overlap threshold\n",
    "    \n",
    "class VDetectorNSA_Binary:\n",
    "    \"\"\"\n",
    "    Binary Negative Selection Algorithm using r-overlap matching rule.\n",
    "    Detectors are binary sparse vectors that should NOT match any self sample\n",
    "    with overlap >= radius.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 k: int,                 # active bits per detector\n",
    "                 r_min: int, r_max: int, # min/max overlap radius\n",
    "                 max_detectors: int,     # how many detectors to accept\n",
    "                 max_tries: int,         # max attempts to generate\n",
    "                 batch_size: int,        # batch size for overlap calc\n",
    "                 sampling: str = \"antiprofile\",\n",
    "                 random_state: int = 42):\n",
    "        self.k = k\n",
    "        self.r_min = r_min\n",
    "        self.r_max = r_max\n",
    "        self.max_detectors = max_detectors\n",
    "        self.max_tries = max_tries\n",
    "        self.batch_size = batch_size\n",
    "        self.sampling = sampling\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.dim = None\n",
    "        self.detectors: list[Detector] = []\n",
    "        self.p_detect = None\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # internal helpers\n",
    "    # --------------------------------------------------------------\n",
    "    def _build_antiprofile_probs(self, X_ham_train: sp.csr_matrix) -> None:\n",
    "        \"\"\"Build antiprofile sampling probabilities from self data.\"\"\"\n",
    "        assert sp.issparse(X_ham_train)\n",
    "        p_ham = (X_ham_train.sum(axis=0) / X_ham_train.shape[0]).A1\n",
    "        p = np.clip(1.0 - p_ham, 1e-8, 1.0)\n",
    "        self.p_detect = p / p.sum()\n",
    "\n",
    "    def _sample_indices(self) -> np.ndarray:\n",
    "        if self.sampling == \"antiprofile\" and self.p_detect is not None:\n",
    "            return np.sort(np.random.choice(self.dim, size=min(self.k, self.dim),\n",
    "                                            replace=False, p=self.p_detect))\n",
    "        return np.sort(np.random.choice(self.dim, size=min(self.k, self.dim),\n",
    "                                        replace=False))\n",
    "\n",
    "    @staticmethod\n",
    "    def _vec_from_idx(idx: np.ndarray, dim: int) -> sp.csr_matrix:\n",
    "        \"\"\"Construct a 1×dim sparse row vector with 1s at idx.\"\"\"\n",
    "        data = np.ones(len(idx), dtype=np.uint8)\n",
    "        rows = np.zeros(len(idx), dtype=np.int32)\n",
    "        return sp.csr_matrix((data, (rows, idx)), shape=(1, dim))\n",
    "\n",
    "    @staticmethod\n",
    "    def _max_overlap(X: sp.csr_matrix, detector_vec: sp.csr_matrix, batch_size: int) -> int:\n",
    "        \"\"\"Return the maximum overlap between detector and any row in X.\"\"\"\n",
    "        best = 0\n",
    "        n = X.shape[0]\n",
    "        for s in range(0, n, batch_size):\n",
    "            e = min(s + batch_size, n)\n",
    "            overlap = (X[s:e] @ detector_vec.T).A.ravel()\n",
    "            if overlap.size:\n",
    "                best = max(best, int(overlap.max()))\n",
    "        return best\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # main API\n",
    "    # --------------------------------------------------------------\n",
    "    def fit(self, X_ham_train: sp.csr_matrix):\n",
    "        assert sp.issparse(X_ham_train)\n",
    "        self.dim = X_ham_train.shape[1]\n",
    "        if self.sampling == \"antiprofile\":\n",
    "            self._build_antiprofile_probs(X_ham_train)\n",
    "\n",
    "        accepted, tries = 0, 0\n",
    "        while accepted < self.max_detectors and tries < self.max_tries:\n",
    "            tries += 1\n",
    "            idx = self._sample_indices()\n",
    "            det_vec = self._vec_from_idx(idx, self.dim)\n",
    "\n",
    "            # choose a random radius within the allowed range\n",
    "            r = np.random.randint(self.r_min, self.r_max + 1)\n",
    "\n",
    "            # reject if detector matches any ham within radius\n",
    "            m_o = self._max_overlap(X_ham_train, det_vec, self.batch_size)\n",
    "            if m_o >= r:      # too similar to self, reject\n",
    "                continue\n",
    "\n",
    "            # accept detector\n",
    "            self.detectors.append(Detector(idx=idx, radius=r))\n",
    "            accepted += 1\n",
    "\n",
    "        print(f\"Generated {accepted} detectors after {tries} tries.\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_bin: sp.csr_matrix, k_hits: int = 1, return_score: bool = False):\n",
    "        \"\"\"Predict non-self (1=spam) vs self (0=ham).\"\"\"\n",
    "        assert self.detectors, \"Model not fitted.\"\n",
    "        n = X_bin.shape[0]\n",
    "        predictions = np.zeros(n, dtype=np.uint8)\n",
    "\n",
    "        # build detector matrix\n",
    "        rows, cols, data, r_list = [], [], [], []\n",
    "        for i, det in enumerate(self.detectors):\n",
    "            cols.extend(det.idx.tolist())\n",
    "            rows.extend([i] * len(det.idx))\n",
    "            data.extend([1] * len(det.idx))\n",
    "            r_list.append(det.radius)\n",
    "        det_matrix = sp.csr_matrix((data, (rows, cols)), shape=(len(self.detectors), self.dim))\n",
    "        r_array = np.array(r_list)\n",
    "\n",
    "        scores = np.zeros(n, dtype=np.int32) if return_score else None\n",
    "        for s in range(0, n, self.batch_size):\n",
    "            e = min(s + self.batch_size, n)\n",
    "            overlaps = (X_bin[s:e] @ det_matrix.T).A\n",
    "            hits = (overlaps >= r_array)\n",
    "            if return_score:\n",
    "                scores[s:e] = hits.sum(axis=1)\n",
    "            if k_hits == 1:\n",
    "                predictions[s:e] = hits.any(axis=1).astype(np.uint8)\n",
    "            else:\n",
    "                predictions[s:e] = (hits.sum(axis=1) >= k_hits).astype(np.uint8)\n",
    "        return predictions if not return_score else (predictions, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07525ffd",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a86f4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to binarize sparse matrix\n",
    "def binarize_sparse_matrix(X: sp.csr_matrix, tau: float= 0.05) -> sp.csr_matrix:\n",
    "    '''\n",
    "    Binarize sparse matrix X using threshold tau.\n",
    "    Values greater than tau are set to 1, others to 0.\n",
    "    '''\n",
    "    assert sp.issparse(X), \"Input matrix must be a sparse matrix.\"\n",
    "    X_bin = X.copy()\n",
    "    X_bin.data = np.where(X_bin.data > tau, 1, 0)\n",
    "    return X_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7899c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xh_train_bin = binarize_sparse_matrix(Xh_train_tfidf, tau=0.02)\n",
    "Xh_val_bin = binarize_sparse_matrix(Xh_val_tfidf, tau=0.02)\n",
    "Xh_test_bin = binarize_sparse_matrix(Xh_test_tfidf, tau=0.02)\n",
    "Xs_val_bin = binarize_sparse_matrix(Xs_val_tfidf, tau=0.02)\n",
    "Xs_test_bin = binarize_sparse_matrix(Xs_test_tfidf, tau=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c5d0b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2000 detectors after 11765 tries.\n"
     ]
    }
   ],
   "source": [
    "nsa = VDetectorNSA_Binary(\n",
    "    k=20,\n",
    "    r_min=1, r_max=3,\n",
    "    max_detectors=2000,\n",
    "    max_tries=50000,\n",
    "    sampling=\"antiprofile\",\n",
    "    random_state=42,\n",
    "    batch_size=1000\n",
    ").fit(Xh_train_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95b1bc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#detectors: 2000  | dim: 17504\n",
      "Mean ham-hit rate per detector: 0.000101\n",
      "Mean spam-hit rate per detector: 0.000346\n",
      "HAM coverage(any hit):  0.1130\n",
      "SPAM coverage(any hit): 0.4799\n"
     ]
    }
   ],
   "source": [
    "def nsa_diagnostics(nsa, Xh_val_bin, Xs_val_bin):\n",
    "    print(f\"#detectors: {len(nsa.detectors)}  | dim: {nsa.dim}\")\n",
    "\n",
    "    # per-detector stats on validation\n",
    "    rows, cols, data, r_list = [], [], [], []\n",
    "    for i, det in enumerate(nsa.detectors):\n",
    "        cols.extend(det.idx.tolist())\n",
    "        rows.extend([i]*len(det.idx))\n",
    "        data.extend([1]*len(det.idx))\n",
    "        r_list.append(det.radius)\n",
    "    D = sp.csr_matrix((data, (rows, cols)), shape=(len(nsa.detectors), nsa.dim))\n",
    "    r = np.array(r_list)\n",
    "\n",
    "    # how many ham/spam each detector hits\n",
    "    ham_hits = (Xh_val_bin @ D.T).A >= r\n",
    "    spam_hits = (Xs_val_bin @ D.T).A >= r\n",
    "    print(f\"Mean ham-hit rate per detector: {ham_hits.mean():.6f}\")\n",
    "    print(f\"Mean spam-hit rate per detector: {spam_hits.mean():.6f}\")\n",
    "\n",
    "    # coverage: fraction of samples hit by at least one detector\n",
    "    ham_cov = ham_hits.any(axis=1).mean()\n",
    "    spam_cov = spam_hits.any(axis=1).mean()\n",
    "    print(f\"HAM coverage(any hit):  {ham_cov:.4f}\")\n",
    "    print(f\"SPAM coverage(any hit): {spam_cov:.4f}\")\n",
    "nsa_diagnostics(nsa, Xh_val_bin, Xs_val_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "889cab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----Validation phase----- \n",
    "we will tune 3 parameters here: k_hits, tau, k and r'''\n",
    "X_eval = sp.vstack([Xh_val_bin, Xs_val_bin])\n",
    "y_val = np.hstack([\n",
    "    np.zeros(Xh_val_bin.shape[0], dtype=np.uint8),\n",
    "    np.ones(Xs_val_bin.shape[0], dtype=np.uint8)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e844f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_hits=1: F1-score=0.5416\n",
      "k_hits=2: F1-score=0.2869\n",
      "k_hits=3: F1-score=0.0545\n",
      "k_hits=5: F1-score=0.0000\n",
      "k_hits=7: F1-score=0.0000\n",
      "k_hits=10: F1-score=0.0000\n",
      "Best k_hits: 1 with F1-score: 0.5416\n"
     ]
    }
   ],
   "source": [
    "best_f1, best_hits = 0, None\n",
    "for hits in [1, 2, 3, 5, 7, 10]:\n",
    "    y_pred = nsa.predict(X_val, k_hits=hits)\n",
    "    report = classification_report(y_val, y_pred, output_dict=True)\n",
    "    f1 = report['1']['f1-score']\n",
    "    print(f\"k_hits={hits}: F1-score={f1:.4f}\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_hits = hits\n",
    "print(f\"Best k_hits: {best_hits} with F1-score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a194e25",
   "metadata": {},
   "source": [
    "### Visualization of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "388ec8ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1338, 2895]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m y_pred, scores \u001b[38;5;241m=\u001b[39m nsa\u001b[38;5;241m.\u001b[39mpredict(Xh_train_bin, k_hits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, return_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# === Confusion Matrix ===\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m cm_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cm, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue Ham\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue Spam\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      7\u001b[0m                      columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPred Ham\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPred Spam\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\welde\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\welde\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:342\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    248\u001b[0m     {\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    (np.int64(0), np.int64(2), np.int64(1), np.int64(1))\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32mc:\\Users\\welde\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\welde\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1338, 2895]"
     ]
    }
   ],
   "source": [
    "# === Predict ===\n",
    "y_pred, scores = nsa.predict(Xh_train_bin, k_hits=1, return_score=True)\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=[\"True Ham\", \"True Spam\"],\n",
    "                     columns=[\"Pred Ham\", \"Pred Spam\"])\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (k_hits=1)\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Classification Report ===\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred, target_names=[\"Ham\", \"Spam\"]))\n",
    "\n",
    "# === Precision–Recall Curve ===\n",
    "prec, rec, _ = precision_recall_curve(y_val, scores)\n",
    "pr_auc = auc(rec, prec)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(rec, prec, color=\"darkorange\", linewidth=2,\n",
    "         label=f\"PR Curve (AUC = {pr_auc:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve (Spam as Positive)\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPrecision–Recall AUC: {pr_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cd181",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hits \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m nsa\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_eval\u001b[49m, k_hits\u001b[38;5;241m=\u001b[39mhits)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Evaluation with k_hits=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     plot_confusion_matrix(y_eval, y_pred, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion Matrix (k_hits=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_eval' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
